{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *CIFAR-10 with CNN*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem Statement\n",
    "\n",
    "This is a classification problem with 10 classes(muti-label classification). The dataset for this task is the CIFAR-10. Each class contains 6,000 images which makes up 60,000 color images in the dataset. The training set contains 50,000 images, while the test sets provides 10,000 images. Link to the dataset: https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Libraries dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import itertools\n",
    "\n",
    "#Hyperparameters\n",
    "batch_size = 32  # The default batch size of keras.\n",
    "num_classes = 10  # Number of class for the dataset\n",
    "epochs = 100\n",
    "data_augmentation = False\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 4s 0us/step\n",
      "x_train shape: (50000, 32, 32, 3)\n",
      "y_train shape: (50000, 1)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# The data, split between train and test sets:\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n",
      "c:\\Users\\DELL\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize = (15,5))\n",
    "#Distribution of Training Set\n",
    "sns.countplot(y_train.ravel(), ax= axs[0])\n",
    "axs[0].set_title(\"Ditribution of Training Dataset\")\n",
    "axs[0].set_xlabel(\"Classes\")\n",
    "#Count Plot for testing Set\n",
    "sns.countplot(y_test.ravel(), ax = axs[1])\n",
    "axs[1].set_title(\"Distribution of Testing Set\")\n",
    "axs[1].set_xlabel(\"Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize data to improve training time\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 30, 30, 32)        9248      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 30, 30, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 15, 15, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 15, 15, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 15, 15, 64)        18496     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 15, 15, 64)        0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 13, 13, 64)        36928     \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 13, 13, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 6, 6, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 6, 6, 64)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2304)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1180160   \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1250858 (4.77 MB)\n",
      "Trainable params: 1250858 (4.77 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Define a convolution net\n",
    "\n",
    "model = Sequential()\n",
    "# CONV => RELU => CONV => RELU => POOL => DROPOUT\n",
    "model.add(Conv2D(32, (3,3), padding = 'same', input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# CONV => RELU => CONV => RELU => POOL => DROPOUT\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# FLATTERN => DENSE => RELU => DROPOUT\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "# a softmax classifier\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.RMSprop(learning_rate=0.0001)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using data augmentation.\n",
      "Epoch 1/100\n",
      "1563/1563 [==============================] - 171s 108ms/step - loss: 1.8571 - accuracy: 0.3241 - val_loss: 1.5740 - val_accuracy: 0.4369\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 1.5214 - accuracy: 0.4501 - val_loss: 1.4194 - val_accuracy: 0.4957\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - 130s 83ms/step - loss: 1.3885 - accuracy: 0.5000 - val_loss: 1.3188 - val_accuracy: 0.5304\n",
      "Epoch 4/100\n",
      "1563/1563 [==============================] - 134s 86ms/step - loss: 1.2902 - accuracy: 0.5407 - val_loss: 1.2123 - val_accuracy: 0.5767\n",
      "Epoch 5/100\n",
      "1563/1563 [==============================] - 136s 87ms/step - loss: 1.2070 - accuracy: 0.5740 - val_loss: 1.0932 - val_accuracy: 0.6194\n",
      "Epoch 6/100\n",
      "1563/1563 [==============================] - 134s 86ms/step - loss: 1.1368 - accuracy: 0.5997 - val_loss: 1.0664 - val_accuracy: 0.6245\n",
      "Epoch 7/100\n",
      "1563/1563 [==============================] - 138s 88ms/step - loss: 1.0811 - accuracy: 0.6191 - val_loss: 1.0794 - val_accuracy: 0.6232\n",
      "Epoch 8/100\n",
      "1563/1563 [==============================] - 187s 120ms/step - loss: 1.0262 - accuracy: 0.6411 - val_loss: 0.9468 - val_accuracy: 0.6667\n",
      "Epoch 9/100\n",
      "1563/1563 [==============================] - 182s 116ms/step - loss: 0.9869 - accuracy: 0.6556 - val_loss: 0.9354 - val_accuracy: 0.6730\n",
      "Epoch 10/100\n",
      "1563/1563 [==============================] - 172s 110ms/step - loss: 0.9485 - accuracy: 0.6691 - val_loss: 0.8780 - val_accuracy: 0.6962\n",
      "Epoch 11/100\n",
      "1563/1563 [==============================] - 146s 93ms/step - loss: 0.9134 - accuracy: 0.6820 - val_loss: 0.9114 - val_accuracy: 0.6842\n",
      "Epoch 12/100\n",
      "1563/1563 [==============================] - 147s 94ms/step - loss: 0.8829 - accuracy: 0.6927 - val_loss: 0.8306 - val_accuracy: 0.7146\n",
      "Epoch 13/100\n",
      "1563/1563 [==============================] - 156s 100ms/step - loss: 0.8558 - accuracy: 0.7027 - val_loss: 0.8215 - val_accuracy: 0.7159\n",
      "Epoch 14/100\n",
      "1563/1563 [==============================] - 158s 101ms/step - loss: 0.8316 - accuracy: 0.7114 - val_loss: 0.7853 - val_accuracy: 0.7286\n",
      "Epoch 15/100\n",
      "1563/1563 [==============================] - 162s 104ms/step - loss: 0.8101 - accuracy: 0.7185 - val_loss: 0.7942 - val_accuracy: 0.7278\n",
      "Epoch 16/100\n",
      "1563/1563 [==============================] - 150s 96ms/step - loss: 0.7885 - accuracy: 0.7258 - val_loss: 0.7683 - val_accuracy: 0.7364\n",
      "Epoch 17/100\n",
      "1563/1563 [==============================] - 167s 107ms/step - loss: 0.7703 - accuracy: 0.7345 - val_loss: 0.7554 - val_accuracy: 0.7401\n",
      "Epoch 18/100\n",
      "1563/1563 [==============================] - 156s 100ms/step - loss: 0.7549 - accuracy: 0.7407 - val_loss: 0.7462 - val_accuracy: 0.7448\n",
      "Epoch 19/100\n",
      "1563/1563 [==============================] - 162s 103ms/step - loss: 0.7402 - accuracy: 0.7456 - val_loss: 0.7303 - val_accuracy: 0.7495\n",
      "Epoch 20/100\n",
      "1563/1563 [==============================] - 188s 120ms/step - loss: 0.7281 - accuracy: 0.7481 - val_loss: 0.7795 - val_accuracy: 0.7418\n",
      "Epoch 21/100\n",
      "1563/1563 [==============================] - 190s 122ms/step - loss: 0.7142 - accuracy: 0.7521 - val_loss: 0.7213 - val_accuracy: 0.7562\n",
      "Epoch 22/100\n",
      "1563/1563 [==============================] - 191s 122ms/step - loss: 0.7033 - accuracy: 0.7574 - val_loss: 0.7155 - val_accuracy: 0.7587\n",
      "Epoch 23/100\n",
      "1563/1563 [==============================] - 189s 121ms/step - loss: 0.6971 - accuracy: 0.7597 - val_loss: 0.7079 - val_accuracy: 0.7585\n",
      "Epoch 24/100\n",
      "1563/1563 [==============================] - 177s 113ms/step - loss: 0.6884 - accuracy: 0.7630 - val_loss: 0.7017 - val_accuracy: 0.7627\n",
      "Epoch 25/100\n",
      "1563/1563 [==============================] - 162s 103ms/step - loss: 0.6796 - accuracy: 0.7678 - val_loss: 0.7002 - val_accuracy: 0.7616\n",
      "Epoch 26/100\n",
      "1563/1563 [==============================] - 162s 104ms/step - loss: 0.6756 - accuracy: 0.7691 - val_loss: 0.6826 - val_accuracy: 0.7691\n",
      "Epoch 27/100\n",
      "1563/1563 [==============================] - 163s 104ms/step - loss: 0.6674 - accuracy: 0.7699 - val_loss: 0.6906 - val_accuracy: 0.7684\n",
      "Epoch 28/100\n",
      "1563/1563 [==============================] - 164s 105ms/step - loss: 0.6586 - accuracy: 0.7762 - val_loss: 0.6840 - val_accuracy: 0.7719\n",
      "Epoch 29/100\n",
      "1563/1563 [==============================] - 163s 104ms/step - loss: 0.6538 - accuracy: 0.7764 - val_loss: 0.6663 - val_accuracy: 0.7730\n",
      "Epoch 30/100\n",
      "1563/1563 [==============================] - 161s 103ms/step - loss: 0.6511 - accuracy: 0.7784 - val_loss: 0.6592 - val_accuracy: 0.7748\n",
      "Epoch 31/100\n",
      "1563/1563 [==============================] - 162s 103ms/step - loss: 0.6468 - accuracy: 0.7789 - val_loss: 0.7386 - val_accuracy: 0.7521\n",
      "Epoch 32/100\n",
      "1563/1563 [==============================] - 165s 105ms/step - loss: 0.6453 - accuracy: 0.7787 - val_loss: 0.6420 - val_accuracy: 0.7808\n",
      "Epoch 33/100\n",
      "1563/1563 [==============================] - 166s 107ms/step - loss: 0.6405 - accuracy: 0.7819 - val_loss: 0.6650 - val_accuracy: 0.7786\n",
      "Epoch 34/100\n",
      "1563/1563 [==============================] - 162s 104ms/step - loss: 0.6375 - accuracy: 0.7835 - val_loss: 0.6612 - val_accuracy: 0.7757\n",
      "Epoch 35/100\n",
      "1563/1563 [==============================] - 162s 104ms/step - loss: 0.6342 - accuracy: 0.7851 - val_loss: 0.6339 - val_accuracy: 0.7921\n",
      "Epoch 36/100\n",
      "1563/1563 [==============================] - 160s 103ms/step - loss: 0.6293 - accuracy: 0.7861 - val_loss: 0.6309 - val_accuracy: 0.7877\n",
      "Epoch 37/100\n",
      "1563/1563 [==============================] - 159s 102ms/step - loss: 0.6286 - accuracy: 0.7873 - val_loss: 0.6712 - val_accuracy: 0.7772\n",
      "Epoch 38/100\n",
      "1563/1563 [==============================] - 160s 102ms/step - loss: 0.6220 - accuracy: 0.7899 - val_loss: 0.6667 - val_accuracy: 0.7836\n",
      "Epoch 39/100\n",
      "1563/1563 [==============================] - 160s 102ms/step - loss: 0.6166 - accuracy: 0.7900 - val_loss: 0.6512 - val_accuracy: 0.7881\n",
      "Epoch 40/100\n",
      "1563/1563 [==============================] - 160s 102ms/step - loss: 0.6173 - accuracy: 0.7925 - val_loss: 0.6557 - val_accuracy: 0.7838\n",
      "Epoch 41/100\n",
      "1563/1563 [==============================] - 160s 102ms/step - loss: 0.6146 - accuracy: 0.7932 - val_loss: 0.6257 - val_accuracy: 0.7902\n",
      "Epoch 42/100\n",
      "1563/1563 [==============================] - 161s 103ms/step - loss: 0.6062 - accuracy: 0.7955 - val_loss: 0.6263 - val_accuracy: 0.7934\n",
      "Epoch 43/100\n",
      "1563/1563 [==============================] - 160s 102ms/step - loss: 0.6043 - accuracy: 0.7948 - val_loss: 0.6332 - val_accuracy: 0.7943\n",
      "Epoch 44/100\n",
      "1563/1563 [==============================] - 162s 104ms/step - loss: 0.6033 - accuracy: 0.7949 - val_loss: 0.6413 - val_accuracy: 0.7924\n",
      "Epoch 45/100\n",
      "1563/1563 [==============================] - 160s 103ms/step - loss: 0.6015 - accuracy: 0.7965 - val_loss: 0.6199 - val_accuracy: 0.7938\n",
      "Epoch 46/100\n",
      "1563/1563 [==============================] - 162s 104ms/step - loss: 0.5966 - accuracy: 0.8012 - val_loss: 0.6591 - val_accuracy: 0.7798\n",
      "Epoch 47/100\n",
      "1563/1563 [==============================] - 160s 103ms/step - loss: 0.5943 - accuracy: 0.7997 - val_loss: 0.6505 - val_accuracy: 0.7826\n",
      "Epoch 48/100\n",
      "1563/1563 [==============================] - 160s 103ms/step - loss: 0.5948 - accuracy: 0.8004 - val_loss: 0.6313 - val_accuracy: 0.7879\n",
      "Epoch 49/100\n",
      "1563/1563 [==============================] - 160s 102ms/step - loss: 0.5915 - accuracy: 0.8014 - val_loss: 0.6311 - val_accuracy: 0.7939\n",
      "Epoch 50/100\n",
      "1563/1563 [==============================] - 159s 102ms/step - loss: 0.5854 - accuracy: 0.8036 - val_loss: 0.6106 - val_accuracy: 0.7986\n",
      "Epoch 51/100\n",
      "1563/1563 [==============================] - 158s 101ms/step - loss: 0.5831 - accuracy: 0.8066 - val_loss: 0.6082 - val_accuracy: 0.7975\n",
      "Epoch 52/100\n",
      "1563/1563 [==============================] - 156s 100ms/step - loss: 0.5813 - accuracy: 0.8035 - val_loss: 0.6136 - val_accuracy: 0.7961\n",
      "Epoch 53/100\n",
      "1563/1563 [==============================] - 142s 91ms/step - loss: 0.5779 - accuracy: 0.8067 - val_loss: 0.6436 - val_accuracy: 0.7902\n",
      "Epoch 54/100\n",
      "1563/1563 [==============================] - 142s 91ms/step - loss: 0.5786 - accuracy: 0.8069 - val_loss: 0.6031 - val_accuracy: 0.7996\n",
      "Epoch 55/100\n",
      "1563/1563 [==============================] - 142s 91ms/step - loss: 0.5763 - accuracy: 0.8060 - val_loss: 0.6367 - val_accuracy: 0.7928\n",
      "Epoch 56/100\n",
      "1563/1563 [==============================] - 141s 90ms/step - loss: 0.5728 - accuracy: 0.8083 - val_loss: 0.6240 - val_accuracy: 0.7953\n",
      "Epoch 57/100\n",
      "1563/1563 [==============================] - 141s 90ms/step - loss: 0.5679 - accuracy: 0.8074 - val_loss: 0.6244 - val_accuracy: 0.7983\n",
      "Epoch 58/100\n",
      "1563/1563 [==============================] - 141s 90ms/step - loss: 0.5716 - accuracy: 0.8083 - val_loss: 0.6114 - val_accuracy: 0.7990\n",
      "Epoch 59/100\n",
      "1563/1563 [==============================] - 142s 91ms/step - loss: 0.5692 - accuracy: 0.8090 - val_loss: 0.6855 - val_accuracy: 0.7741\n",
      "Epoch 60/100\n",
      "1563/1563 [==============================] - 141s 91ms/step - loss: 0.5670 - accuracy: 0.8111 - val_loss: 0.6463 - val_accuracy: 0.7848\n",
      "Epoch 61/100\n",
      "1563/1563 [==============================] - 133s 85ms/step - loss: 0.5636 - accuracy: 0.8118 - val_loss: 0.6487 - val_accuracy: 0.7865\n",
      "Epoch 62/100\n",
      "1563/1563 [==============================] - 119s 76ms/step - loss: 0.5660 - accuracy: 0.8107 - val_loss: 0.6551 - val_accuracy: 0.7831\n",
      "Epoch 63/100\n",
      "1563/1563 [==============================] - 119s 76ms/step - loss: 0.5639 - accuracy: 0.8093 - val_loss: 0.6285 - val_accuracy: 0.7926\n",
      "Epoch 64/100\n",
      "1563/1563 [==============================] - 119s 76ms/step - loss: 0.5582 - accuracy: 0.8118 - val_loss: 0.6249 - val_accuracy: 0.7944\n",
      "Epoch 65/100\n",
      "1563/1563 [==============================] - 118s 76ms/step - loss: 0.5557 - accuracy: 0.8141 - val_loss: 0.6097 - val_accuracy: 0.8001\n",
      "Epoch 66/100\n",
      "1563/1563 [==============================] - 119s 76ms/step - loss: 0.5510 - accuracy: 0.8167 - val_loss: 0.6290 - val_accuracy: 0.7935\n",
      "Epoch 67/100\n",
      "1563/1563 [==============================] - 120s 76ms/step - loss: 0.5535 - accuracy: 0.8114 - val_loss: 0.6185 - val_accuracy: 0.7974\n",
      "Epoch 68/100\n",
      "1563/1563 [==============================] - 119s 76ms/step - loss: 0.5585 - accuracy: 0.8152 - val_loss: 0.6432 - val_accuracy: 0.7915\n",
      "Epoch 69/100\n",
      "1563/1563 [==============================] - 119s 76ms/step - loss: 0.5549 - accuracy: 0.8148 - val_loss: 0.6365 - val_accuracy: 0.7873\n",
      "Epoch 70/100\n",
      "1563/1563 [==============================] - 118s 76ms/step - loss: 0.5549 - accuracy: 0.8138 - val_loss: 0.6185 - val_accuracy: 0.7974\n",
      "Epoch 71/100\n",
      "1563/1563 [==============================] - 7781s 5s/step - loss: 0.5510 - accuracy: 0.8174 - val_loss: 0.6675 - val_accuracy: 0.7837\n",
      "Epoch 72/100\n",
      "1563/1563 [==============================] - 133s 85ms/step - loss: 0.5498 - accuracy: 0.8183 - val_loss: 0.6236 - val_accuracy: 0.8024\n",
      "Epoch 73/100\n",
      "1563/1563 [==============================] - 133s 85ms/step - loss: 0.5486 - accuracy: 0.8170 - val_loss: 0.6310 - val_accuracy: 0.7911\n",
      "Epoch 74/100\n",
      "1563/1563 [==============================] - 124s 79ms/step - loss: 0.5510 - accuracy: 0.8164 - val_loss: 0.6442 - val_accuracy: 0.7963\n",
      "Epoch 75/100\n",
      "1563/1563 [==============================] - 129s 82ms/step - loss: 0.5436 - accuracy: 0.8179 - val_loss: 0.6771 - val_accuracy: 0.7818\n",
      "Epoch 76/100\n",
      "1563/1563 [==============================] - 130s 83ms/step - loss: 0.5420 - accuracy: 0.8204 - val_loss: 0.6172 - val_accuracy: 0.7990\n",
      "Epoch 77/100\n",
      "1563/1563 [==============================] - 149s 96ms/step - loss: 0.5398 - accuracy: 0.8193 - val_loss: 0.6471 - val_accuracy: 0.7968\n",
      "Epoch 78/100\n",
      "1563/1563 [==============================] - 130s 83ms/step - loss: 0.5454 - accuracy: 0.8187 - val_loss: 0.5986 - val_accuracy: 0.8044\n",
      "Epoch 79/100\n",
      "1563/1563 [==============================] - 133s 85ms/step - loss: 0.5442 - accuracy: 0.8198 - val_loss: 0.6115 - val_accuracy: 0.7982\n",
      "Epoch 80/100\n",
      "1563/1563 [==============================] - 157s 100ms/step - loss: 0.5386 - accuracy: 0.8199 - val_loss: 0.6082 - val_accuracy: 0.7997\n",
      "Epoch 81/100\n",
      "1563/1563 [==============================] - 155s 99ms/step - loss: 0.5387 - accuracy: 0.8222 - val_loss: 0.6001 - val_accuracy: 0.7992\n",
      "Epoch 82/100\n",
      "1563/1563 [==============================] - 150s 96ms/step - loss: 0.5377 - accuracy: 0.8197 - val_loss: 0.6078 - val_accuracy: 0.8004\n",
      "Epoch 83/100\n",
      "1563/1563 [==============================] - 159s 102ms/step - loss: 0.5378 - accuracy: 0.8200 - val_loss: 0.6016 - val_accuracy: 0.8045\n",
      "Epoch 84/100\n",
      "1563/1563 [==============================] - 133s 85ms/step - loss: 0.5385 - accuracy: 0.8207 - val_loss: 0.5926 - val_accuracy: 0.8053\n",
      "Epoch 85/100\n",
      "1563/1563 [==============================] - 156s 100ms/step - loss: 0.5358 - accuracy: 0.8207 - val_loss: 0.6374 - val_accuracy: 0.7883\n",
      "Epoch 86/100\n",
      "1563/1563 [==============================] - 151s 97ms/step - loss: 0.5284 - accuracy: 0.8234 - val_loss: 0.6451 - val_accuracy: 0.7921\n",
      "Epoch 87/100\n",
      "1563/1563 [==============================] - 150s 96ms/step - loss: 0.5301 - accuracy: 0.8232 - val_loss: 0.6396 - val_accuracy: 0.7937\n",
      "Epoch 88/100\n",
      "1563/1563 [==============================] - 152s 97ms/step - loss: 0.5308 - accuracy: 0.8233 - val_loss: 0.6123 - val_accuracy: 0.7997\n",
      "Epoch 89/100\n",
      "1563/1563 [==============================] - 152s 97ms/step - loss: 0.5313 - accuracy: 0.8230 - val_loss: 0.6375 - val_accuracy: 0.7911\n",
      "Epoch 90/100\n",
      "1563/1563 [==============================] - 132s 84ms/step - loss: 0.5311 - accuracy: 0.8215 - val_loss: 0.6543 - val_accuracy: 0.7857\n",
      "Epoch 91/100\n",
      "1563/1563 [==============================] - 148s 94ms/step - loss: 0.5243 - accuracy: 0.8243 - val_loss: 0.6160 - val_accuracy: 0.7979\n",
      "Epoch 92/100\n",
      "1563/1563 [==============================] - 150s 96ms/step - loss: 0.5286 - accuracy: 0.8232 - val_loss: 0.6218 - val_accuracy: 0.7944\n",
      "Epoch 93/100\n",
      "1563/1563 [==============================] - 153s 98ms/step - loss: 0.5302 - accuracy: 0.8243 - val_loss: 0.6274 - val_accuracy: 0.7997\n",
      "Epoch 94/100\n",
      "1563/1563 [==============================] - 182s 116ms/step - loss: 0.5223 - accuracy: 0.8259 - val_loss: 0.6134 - val_accuracy: 0.8033\n",
      "Epoch 95/100\n",
      "1563/1563 [==============================] - 187s 120ms/step - loss: 0.5241 - accuracy: 0.8277 - val_loss: 0.5973 - val_accuracy: 0.8072\n",
      "Epoch 96/100\n",
      "1563/1563 [==============================] - 182s 117ms/step - loss: 0.5213 - accuracy: 0.8250 - val_loss: 0.6190 - val_accuracy: 0.8025\n",
      "Epoch 97/100\n",
      "1563/1563 [==============================] - 162s 104ms/step - loss: 0.5226 - accuracy: 0.8245 - val_loss: 0.6471 - val_accuracy: 0.7961\n",
      "Epoch 98/100\n",
      "1563/1563 [==============================] - 139s 89ms/step - loss: 0.5179 - accuracy: 0.8286 - val_loss: 0.6131 - val_accuracy: 0.8009\n",
      "Epoch 99/100\n",
      "1563/1563 [==============================] - 151s 96ms/step - loss: 0.5209 - accuracy: 0.8270 - val_loss: 0.6386 - val_accuracy: 0.7893\n",
      "Epoch 100/100\n",
      "1563/1563 [==============================] - 155s 99ms/step - loss: 0.5151 - accuracy: 0.8283 - val_loss: 0.6387 - val_accuracy: 0.7887\n"
     ]
    }
   ],
   "source": [
    "history = None  # For recording the history of trainning process.\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    history = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    history = model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                    batch_size=batch_size),\n",
    "                                    epochs=epochs,\n",
    "                                    validation_data=(x_test, y_test),\n",
    "                                    workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
